{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "train=pd.read_csv(\"engtamilTrain.csv\")\n",
    "train=train.drop([\"Unnamed: 0\"],axis=1)\n",
    "english_sentences=train[\"en\"]\n",
    "tamil_sentence=train['ta']\n",
    "english_sentences=english_sentences.head(1000)\n",
    "tamil_sentences=tamil_sentence.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to add SOS and EOS to the statement\n",
    "\n",
    "def addSosEos(seriesSentence):\n",
    "    # Define the <SOS> and <EOS> tokens\n",
    "    sos_token = \"<SOS>\"\n",
    "    eos_token = \"<EOS>\"\n",
    "\n",
    "    # Add <SOS> and <EOS> tokens to each statement\n",
    "    statements_with_tokens = [f\"{sos_token} {statement} {eos_token}\" for statement in seriesSentence]\n",
    "\n",
    "    english_sent=[]\n",
    "    # Print the statements with tokens\n",
    "    for statement in statements_with_tokens:\n",
    "        english_sent.append(statement)\n",
    "        print(statement)\n",
    "    return english_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sent_SE=addSosEos(english_sentences)\n",
    "tamil_sent_SE=addSosEos(tamil_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the English and Tamil sentences\n",
    "english_tokenizer = Tokenizer(filters=\"\")\n",
    "english_tokenizer.fit_on_texts(english_sent_SE)\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "english_sequences = english_tokenizer.texts_to_sequences(english_sent_SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamil_tokenizer = Tokenizer(filters=\"\")\n",
    "tamil_tokenizer.fit_on_texts(tamil_sent_SE)\n",
    "tamil_vocab_size = len(tamil_tokenizer.word_index) + 1\n",
    "tamil_sequences = tamil_tokenizer.texts_to_sequences(tamil_sent_SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_seq_length=20\n",
    "max_output_seq_length=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length\n",
    "input_sequences = pad_sequences(english_sequences, maxlen=max_input_seq_length, padding='post')\n",
    "output_sequences = pad_sequences(tamil_sequences, maxlen=max_output_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the decoder input and output sequences for teacher forcing\n",
    "decoder_input_sequences = np.zeros_like(output_sequences)\n",
    "decoder_input_sequences[:, 1:] = output_sequences[:, :-1]\n",
    "decoder_input_sequences[:, 0] = tamil_tokenizer.word_index['<sos>']\n",
    "\n",
    "decoder_output_sequences = np.eye(tamil_vocab_size)[output_sequences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "eng_model = Word2Vec.load('engmodel.bin')\n",
    "tam_model = Word2Vec.load('tammodel.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word2vec_model, tokenizer, vocab_size):\n",
    "    embedding_matrix = np.zeros((vocab_size, word2vec_model.vector_size))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word2vec_model.wv[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            pass  # Words not found in the embedding index will be all zeros\n",
    "    return embedding_matrix\n",
    "\n",
    "eng_embedding_matrix = create_embedding_matrix(eng_model, english_tokenizer, english_vocab_size)\n",
    "tam_embedding_matrix = create_embedding_matrix(tam_model, tamil_tokenizer, tamil_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tam_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq2seq_model(input_vocab_size, output_vocab_size, input_seq_length, output_seq_length, hidden_units, eng_embedding_matrix, tam_embedding_matrix):\n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(input_seq_length,))\n",
    "    encoder_embedding = Embedding(input_vocab_size, hidden_units, weights=[eng_embedding_matrix], trainable=False)(encoder_inputs)\n",
    "    encoder_lstm, encoder_state_h, encoder_state_c = LSTM(hidden_units, return_state=True)(encoder_embedding)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(output_seq_length,))\n",
    "    decoder_embedding = Embedding(output_vocab_size, hidden_units, weights=[tam_embedding_matrix], trainable=False)(decoder_inputs)\n",
    "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[encoder_state_h, encoder_state_c])\n",
    "    decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target_sequences to one-hot encoded format\n",
    "target_sequences = tf.keras.utils.to_categorical(output_sequences, num_classes=tamil_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_seq2seq_model(english_vocab_size, tamil_vocab_size, max_input_seq_length, max_output_seq_length, 100, eng_embedding_matrix, tam_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "model.fit([input_sequences, output_sequences], decoder_output_sequences, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the input\n",
    "input_sentence = \"<sos>Finally, the columnist fails to tell us who among the political leaders of the bourgeoisie, past and present, he counts among the paragons of morality<eos>\"\n",
    "\n",
    "# Convert the input sentence to sequence\n",
    "input_sequence = english_tokenizer.texts_to_sequences([input_sentence])\n",
    "\n",
    "# Pad the statement to the maximum input sequence length\n",
    "input_sequence = pad_sequences(input_sequence, maxlen=max_input_seq_length, padding='post')\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict([input_sequence, np.zeros((1, max_output_seq_length))])\n",
    "\n",
    "# Convert predictions to tokens\n",
    "predicted_tokens = np.argmax(predictions, axis=-1)[0]\n",
    "\n",
    "# Create index to word mapping for Tamil vocabulary\n",
    "tamil_index_word = {i: w for w, i in tamil_tokenizer.word_index.items()}\n",
    "\n",
    "\n",
    "# Convert tokens to text\n",
    "decoded_sentence = []\n",
    "for token in predicted_tokens:\n",
    "    if token == 0:  # Assuming 0 is the padding token\n",
    "        continue\n",
    "    word = tamil_index_word.get(token)\n",
    "    if word == '<eos>':\n",
    "        break\n",
    "    if word is not None:\n",
    "        decoded_sentence.append(word)\n",
    "    else:\n",
    "        decoded_sentence.append('<unk>')\n",
    "\n",
    "# Join the words to form the decoded statement\n",
    "decoded_statement = ' '.join(decoded_sentence)\n",
    "\n",
    "# Print the decoded statement\n",
    "print(decoded_statement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
